# Stochastic Gradient Descent
- GD는 기본적으로 기울기를 하강시키면 좋은 결과로 갈 수 있다는 알고리즘
- 이 기울기를 구하는 데이터에 따라 GD의 종류가 약간 바뀔 수 있음
- SGD는 한번에 전체 데이터가 아닌 B개 씩을 계산하고 그 정보로 훈련을 진행함
- 그리고 매번 어떤 데이터들을 쓸 지 전체 데이터 중 무작위로 뽑아 사용함
- 배치 크기가 클수록 전체 데이터를 보는 것에 가까워지고, 최종 수렴 성능이 더 좋을 수 있음
- 너무 작은 배치 크기는 데이터를 잘 반영하지 못하고 너무 업데이트가 잦아 오히려 훈련도 안되고 시간도 오래 걸릴 수 있음
