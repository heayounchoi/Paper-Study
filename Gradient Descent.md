# Gradient Descent
- GD 방식으로는 반드시 local minimum에 도달할 수 있음. 하지만 global minimum으로 갈 수 있다는 보장은 없음
- learning rate를 상수로 두어도 기울기가 크면 많이 움직이고 기울기가 작으면 적게 움직임
- 더이상 x가 변하지 않는 점은 미분한 값이 0이 되는 점으로, local minimum에 도달했다는 것을 알 수 있음
- 보통 learning rate으로 0.01, 0.005, 0.001 정도의 값을 사용함
- Nonconvex optimization
    - 어떤 함수가 모든 정의역에서 볼록하면 convex function, 오목하면 concave function이라고 함
    - 이런 함수들은 전체 최대값/최소값이 곧 극대값/극소값이 됨
    - 이런 함수들에서는 GD가 아주 잘 작동하기 때문에 GD만 계속 반복해서 수행하더라도 무조건 원하는 최소값에 도달할 수 있음
    - 하지만 이런 함수는 아주 드물고, 딥러닝에서 다루는 함수는 늘 nonconvex function임
    - GD가 잘 동작하도록 하기 위해 할 수 있는 일
        - 다양한 시작점에서 시작해보기
        - 학습 속도 변경해보기(어느 정도 성능이 수렴하면 학습 속도를 낮춰 좀 더 촘촘히 움직여 보는 것도 좋음)
- Saddle point problem
    - 기울기가 0이 되는 점은 크게 세 가지: 극대, 극소, 안장점(saddle point)
    - 연구에서는 딥러닝에서 기울기가 0인 점들은 거의 대부분 안장점이기 때문에 탈출 방법만 잘 만들면 탈출할 수 있다고 함
    - 또한 엄청나게 많은 극소값들이 존재하지만, 대부분 극소갑셍서의 성능이 전체 최소값에서의 성능과 거의 비슷할 것이기 때문에 GD가 큰 문제가 되지 않는다고 이야기 함
    - 일차 미분을 사용하는 것이 아니라 **Hessian 행렬을 사용하는 2차 미분 및 뉴턴 하강법** 등을 사용하면 안장점도 전혀 문제없이 최소가 되는 방향으로 이동할 수 있음. 하지만 2차 미분을 구하는 것은 훨씬 복잡함
- Lipschitz Assumption
    - 생각해보면 GD가 이상하게 느껴질 수도 있음
    - 변수들의 상태 A를 기준으로 성능을 측정하고, 그 성능이 좋아지도록 변수들을 바꾸면 그 순간 상태 B가 될 텐데, 거기서도 성능이 이전보다 좋아질 것이라고 말할 수 있을까?
    - **인공신경망은 거의 언제나 Lipschitz 가정을 만족하는 함수**
    - Lipschitz 가정: 입력의 아주 작은 변화가 출력의 아주 작은 변화로 나타난다는 것
